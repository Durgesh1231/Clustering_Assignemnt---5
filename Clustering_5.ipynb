{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
        "# Answer:\n",
        "# A **contingency matrix** (also known as a confusion matrix) is a table used to summarize the performance of a classification model.\n",
        "# It compares the predicted labels to the true labels of the dataset. A typical contingency matrix for binary classification consists of four key values:\n",
        "# - True Positives (TP): The number of correct positive predictions.\n",
        "# - True Negatives (TN): The number of correct negative predictions.\n",
        "# - False Positives (FP): The number of incorrect positive predictions.\n",
        "# - False Negatives (FN): The number of incorrect negative predictions.\n",
        "# It is used to compute various metrics such as accuracy, precision, recall, and F1-score, which help in evaluating model performance.\n",
        "\n",
        "# Example of a confusion matrix:\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# True labels and predicted labels\n",
        "y_true = [1, 0, 1, 1, 0, 0, 1, 0, 1, 0]\n",
        "y_pred = [1, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Output: Confusion matrix array:\n",
        "# [[3 1]   # 3 True Negatives, 1 False Positive\n",
        "#  [1 5]]  # 1 False Negative, 5 True Positives\n",
        "\n",
        "# Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?\n",
        "# Answer:\n",
        "# A **pair confusion matrix** is used in multi-class classification problems to track the number of pairs of instances from different classes that are misclassified.\n",
        "# It measures the similarity and dissimilarity between class pairs. It is useful when the goal is to evaluate how well the model distinguishes between specific class pairs.\n",
        "# In contrast, a regular confusion matrix summarizes the counts of correct and incorrect classifications for each class separately.\n",
        "\n",
        "# Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?\n",
        "# Answer:\n",
        "# An **extrinsic measure** in natural language processing (NLP) is used to evaluate the performance of a language model based on its impact or effectiveness in solving a specific task.\n",
        "# It is typically task-oriented, such as measuring the model's performance on downstream tasks like text classification, machine translation, or information retrieval.\n",
        "# Examples of extrinsic measures include accuracy, BLEU score, and F1 score for NLP tasks.\n",
        "\n",
        "# Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?\n",
        "# Answer:\n",
        "# An **intrinsic measure** evaluates the quality of a model without external tasks. It is typically used to assess how well the model fits the training data or the inherent quality of the model itself.\n",
        "# Examples include perplexity and log-likelihood in language modeling or clustering metrics like silhouette score for unsupervised learning.\n",
        "# **Extrinsic measures**, in contrast, evaluate the model based on its performance on real-world tasks or applications.\n",
        "\n",
        "# Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?\n",
        "# Answer:\n",
        "# The **confusion matrix** helps evaluate the classification model by displaying the counts of true and false positives/negatives. It provides detailed insights into:\n",
        "# - The **accuracy** of the model.\n",
        "# - The **precision** and **recall** for each class.\n",
        "# - Identifying **misclassified samples**, enabling understanding of where the model is making errors.\n",
        "# The confusion matrix can highlight whether the model struggles with specific classes or if there is class imbalance.\n",
        "\n",
        "# Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?\n",
        "# Answer:\n",
        "# Common **intrinsic measures** for evaluating unsupervised learning algorithms (such as clustering) include:\n",
        "# - **Silhouette score**: Measures how similar an object is to its own cluster compared to other clusters. Ranges from -1 (incorrect clustering) to 1 (well-clustered).\n",
        "# - **Davies-Bouldin Index**: Measures the compactness and separation of clusters. Lower values indicate better clustering.\n",
        "# - **Calinski-Harabasz Index**: Measures the ratio of the sum of between-cluster dispersion to within-cluster dispersion. Higher values indicate better clustering.\n",
        "# These metrics help assess the quality of clustering without the need for ground truth labels.\n",
        "\n",
        "# Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?\n",
        "# Answer:\n",
        "# **Limitations of Accuracy**:\n",
        "# - **Class imbalance**: Accuracy can be misleading in cases where the dataset has a class imbalance (e.g., 95% negatives and 5% positives).\n",
        "# - **Does not account for misclassifications**: A model could have a high accuracy but still perform poorly on certain classes (e.g., predicting only the majority class).\n",
        "# - **Ignoring false positives/negatives**: Accuracy doesn't provide information about the types of errors made by the model.\n",
        "\n",
        "# **Solutions**:\n",
        "# - Use additional metrics such as **precision**, **recall**, **F1-score**, or **AUC-ROC** to get a more comprehensive evaluation.\n",
        "# - In cases of class imbalance, consider using **balanced accuracy** or **confusion matrix** analysis to better understand model performance.\n"
      ]
    }
  ]
}